{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiWbs8uOlleO",
        "outputId": "a4bcacb1-7618-41f4-c177-05fc5b5c83ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.\"\n"
      ],
      "metadata": {
        "id": "_75S5B3SlqJB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#word tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokenize = word_tokenize(text)\n",
        "print(tokenize)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIsHKCT_mPJg",
        "outputId": "9b9dfd3e-eef9-4ce6-f62c-1fccfd36d5dd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization', 'is', 'the', 'first', 'step', 'in', 'text', 'analytics', '.', 'The', 'process', 'of', 'breaking', 'down', 'a', 'text', 'paragraph', 'into', 'smaller', 'chunks', 'such', 'as', 'words', 'or', 'sentences', 'is', 'called', 'Tokenization', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sentence tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "token = sent_tokenize(text)\n",
        "print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NxJRfJnmSm5",
        "outputId": "c8e2e614-e22b-4b2c-d191-26577c70b006"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization is the first step in text analytics.', 'The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stop word\n",
        "import re\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "vXt-foQgnBlq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_word = set(stopwords.words(\"english\"))\n",
        "print(stop_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUmCR2IvnWVF",
        "outputId": "521d6c8e-168c-4ba0-9792-42b020e59950"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'won', 'if', 'she', 'will', 'whom', 'some', 'theirs', 'our', 'about', 'now', 'each', 'yourself', \"shan't\", \"won't\", 'haven', \"you'd\", 'up', 'himself', 'what', 'is', 'on', 'that', \"you're\", 'here', 'when', 'has', 'it', 'y', 'but', 'or', 'below', 'while', 'itself', 'such', 'isn', \"wasn't\", 'we', 'more', \"don't\", 'yours', 'wouldn', 'into', 'he', 'been', \"should've\", 'hasn', 're', 'can', 'd', 'being', 'again', 'didn', \"shouldn't\", 'should', 'who', 'and', 'where', 'why', \"it's\", 'there', 'during', 'shan', 'all', \"you've\", 'your', 'mightn', 'those', 'by', \"wouldn't\", 'hadn', 'them', 'of', 'm', \"doesn't\", \"she's\", 'doesn', 'hers', 'do', 'no', 'me', 'its', 'only', 'my', 'her', 'this', \"that'll\", \"hadn't\", 'too', \"mightn't\", 'needn', 'having', 've', 'an', 'herself', 'these', 'against', 'the', 'have', 'themselves', 'very', 'in', 'then', 'shouldn', \"didn't\", 'his', 'any', \"aren't\", \"weren't\", 'own', 'from', 'did', 'ours', 'at', 'him', 'between', 'with', 'doing', 'other', 'through', 'couldn', 'before', 'are', 'had', 'they', \"isn't\", 'than', 'as', 'to', 'wasn', 'weren', 'does', 'under', 'their', 'few', 'so', 'once', 'you', 'aren', 'don', \"hasn't\", 't', 'not', 'nor', 'how', 'above', 'a', 'over', \"needn't\", 'am', \"you'll\", 'o', 'same', 'ma', 'ourselves', \"couldn't\", 'out', \"mustn't\", 'after', 'off', 'll', 'ain', 'just', 'were', 'until', 's', 'most', 'because', 'for', 'mustn', 'down', 'which', 'both', \"haven't\", 'i', 'be', 'further', 'yourselves', 'myself', 'was'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"How to remove stop words with NLTK library in Python?\"\n",
        "text = re.sub(\"[^a-zA-Z]\" , \" \",text)\n",
        "print(text)\n",
        "token = word_tokenize(text.lower())\n",
        "filter = [w for w in token if w not in stop_word]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUsbMKiJnqgb",
        "outputId": "f3e7eb15-39ad-442f-f784-b1bac86c2acb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How to remove stop words with NLTK library in Python \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokenized Sentence:\",token)\n",
        "print(\"Filterd Sentence:\",filter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vK2oMrJodud",
        "outputId": "06fa1aad-354c-4ac9-fb05-aa0ff0304eb5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Sentence: ['how', 'to', 'remove', 'stop', 'words', 'with', 'nltk', 'library', 'in', 'python']\n",
            "Filterd Sentence: ['remove', 'stop', 'words', 'nltk', 'library', 'python']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "echo_w = [\"watt\" , \"waiting\" , \"waited\"]\n",
        "ps = PorterStemmer()\n",
        "stemmer = [ps.stem(w) for w in echo_w]\n",
        "print(stemmer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbQaPRP0pSYM",
        "outputId": "3de8fa90-6965-444a-ba3e-0bb784db373e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['watt', 'wait', 'wait']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "ps = WordNetLemmatizer()\n",
        "text = \"basbd niason nsandas saidkas\"\n",
        "tokenize = word_tokenize(text)\n",
        "for w in tokenize:\n",
        "  print(\"Lemma is {} for {}\".format(w , ps.lemmatize(w)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uo56jqnKywvL",
        "outputId": "ca8b9cfb-a783-48ac-8602-c28ef93ab4af"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemma is basbd for basbd\n",
            "Lemma is niason for niason\n",
            "Lemma is nsandas for nsandas\n",
            "Lemma is saidkas for saidkas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text =\"The pink sweater fit her perfectly 4\"\n",
        "tokenize = word_tokenize(text)\n",
        "pos = nltk.pos_tag(tokenize)\n",
        "print(pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpchBdpezjTU",
        "outputId": "76e61772-3afc-454e-c545-e3a792bc5f82"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('pink', 'NN'), ('sweater', 'NN'), ('fit', 'VBP'), ('her', 'PRP$'), ('perfectly', 'RB'), ('4', 'CD')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y49pIwsN1rII",
        "outputId": "c087fa19-5d71-4532-fccd-daf6cfa4cafe"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT')]\n",
            "[('pink', 'NN')]\n",
            "[('sweater', 'NN')]\n",
            "[('fit', 'NN')]\n",
            "[('her', 'PRP$')]\n",
            "[('perfectly', 'RB')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "document = [\n",
        "    'Jupiter is the largest Planet',\n",
        "    'Mars is the fourth planet from the Sun'\n",
        "]\n"
      ],
      "metadata": {
        "id": "ATPHmU_H8CPw"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_vectorize = TfidfVectorizer(use_idf = False)"
      ],
      "metadata": {
        "id": "CDV9yE9yAMyB"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_matrix = tf_vectorize.fit_transform(document)\n"
      ],
      "metadata": {
        "id": "41f0jLcAAwgG"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_df = pd.DataFrame(tf_matrix.toarray() , columns = tf_vectorize.get_feature_names_out())\n",
        "print(tf_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kePJ2YRIA5Yp",
        "outputId": "115a3028-6d0c-4bcc-e897-6556d0c4090e"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     fourth      from        is   jupiter   largest      mars    planet  \\\n",
            "0  0.000000  0.000000  0.447214  0.447214  0.447214  0.000000  0.447214   \n",
            "1  0.316228  0.316228  0.316228  0.000000  0.000000  0.316228  0.316228   \n",
            "\n",
            "        sun       the  \n",
            "0  0.000000  0.447214  \n",
            "1  0.316228  0.632456  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nMAmP2DwCRDr"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "itf_vect = TfidfVectorizer(use_idf = True)\n",
        "idf_matrix = itf_vect.fit_transform(document)\n"
      ],
      "metadata": {
        "id": "lpkQgs6mCaHz"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idf_df = pd.DataFrame(idf_matrix.toarray() , columns = itf_vect.get_feature_names_out())"
      ],
      "metadata": {
        "id": "0EBpiyQpDEBI"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(idf_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyLMrUtWDY3_",
        "outputId": "8bcce1d0-661f-4430-b55a-bbb7b26d94ad"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     fourth      from        is   jupiter   largest      mars    planet  \\\n",
            "0  0.000000  0.000000  0.379303  0.533098  0.533098  0.000000  0.379303   \n",
            "1  0.376957  0.376957  0.268208  0.000000  0.000000  0.376957  0.268208   \n",
            "\n",
            "        sun       the  \n",
            "0  0.000000  0.379303  \n",
            "1  0.376957  0.536416  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kRghgfHcDuhM"
      },
      "execution_count": 60,
      "outputs": []
    }
  ]
}